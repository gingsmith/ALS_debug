[0m[[0minfo[0m] [0mLoading project definition from /mnt/ALS_debug/project[0m
[0m[[0minfo[0m] [0mSet current project to als_debug (in build file:/mnt/ALS_debug/)[0m
[0m[[0minfo[0m] [0mRunning als_debug.DFC_ALS --master=spark://ec2-50-19-146-75.compute-1.amazonaws.com:7077 --datadir=data/ --jars=/mnt/ALS_debug/target/als_debug-assembly-1.0.jar --sparkhome=/root/spark --train=eachmovie_randSplit1_data.txt --test=eachmovie_randSplit1_testData.txt --m=1648 --n=30000 --rank=20 --maxiter=10 --nsplits=4[0m
master:       spark://ec2-50-19-146-75.compute-1.amazonaws.com:7077
datadir:      data/
train:    eachmovie_randSplit1_data.txt
test:     eachmovie_randSplit1_testData.txt
rank:         20
lambda:       0.01
maxiter:        10
jar:          /mnt/ALS_debug/target/als_debug-assembly-1.0.jar
sparkhome:    /root/spark
nsplits:    4
m:    1648
n:    30000
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
Filled Mmatrix
Filled Lmatrix
about to make context
13/06/10 07:13:43 INFO Slf4jEventHandler: Slf4jEventHandler started
13/06/10 07:13:43 INFO BlockManagerMaster: Registered BlockManagerMaster Actor
13/06/10 07:13:43 INFO MemoryStore: MemoryStore started with capacity 18.8 GB.
13/06/10 07:13:43 INFO DiskStore: Created local directory at /tmp/spark-local-20130610071343-8f86
13/06/10 07:13:43 INFO ConnectionManager: Bound socket to port 50900 with id = ConnectionManagerId(ip-10-171-2-179.ec2.internal,50900)
13/06/10 07:13:43 INFO BlockManagerMaster: Trying to register BlockManager
13/06/10 07:13:43 INFO BlockManagerMaster: Registered BlockManager
13/06/10 07:13:43 INFO HttpBroadcast: Broadcast server started at http://10.171.2.179:49519
13/06/10 07:13:43 INFO MapOutputTracker: Registered MapOutputTrackerActor actor
13/06/10 07:13:43 INFO HttpFileServer: HTTP File server directory is /tmp/spark-5f76d670-990e-4ff1-8303-94df4f7856e8
13/06/10 07:13:43 INFO IoWorker: IoWorker thread 'spray-io-worker-0' started
13/06/10 07:13:43 INFO HttpServer: akka://spark/user/BlockManagerHTTPServer started on /0.0.0.0:60743
13/06/10 07:13:43 INFO BlockManagerUI: Started BlockManager web UI at http://ip-10-171-2-179.ec2.internal:60743
13/06/10 07:13:43 INFO SparkContext: Added JAR /mnt/ALS_debug/target/als_debug-assembly-1.0.jar at http://10.171.2.179:41669/jars/als_debug-assembly-1.0.jar with timestamp 1370848423992
13/06/10 07:13:44 INFO Client$ClientActor: Connecting to master spark://ec2-50-19-146-75.compute-1.amazonaws.com:7077
made context
13/06/10 07:13:44 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20130610071344-0000
13/06/10 07:13:44 INFO Client$ClientActor: Executor added: app-20130610071344-0000/0 on worker-20130610071332-ip-10-232-20-48.ec2.internal-54209 (ip-10-232-20-48.ec2.internal) with 8 cores
13/06/10 07:13:44 INFO SparkDeploySchedulerBackend: Granted executor ID app-20130610071344-0000/0 on host ip-10-232-20-48.ec2.internal with 8 cores, 40.0 GB RAM
13/06/10 07:13:44 INFO Client$ClientActor: Executor added: app-20130610071344-0000/1 on worker-20130610071332-ip-10-171-11-51.ec2.internal-37240 (ip-10-171-11-51.ec2.internal) with 8 cores
13/06/10 07:13:44 INFO SparkDeploySchedulerBackend: Granted executor ID app-20130610071344-0000/1 on host ip-10-171-11-51.ec2.internal with 8 cores, 40.0 GB RAM
13/06/10 07:13:44 INFO Client$ClientActor: Executor added: app-20130610071344-0000/2 on worker-20130610071332-ip-10-170-19-137.ec2.internal-39622 (ip-10-170-19-137.ec2.internal) with 8 cores
13/06/10 07:13:44 INFO SparkDeploySchedulerBackend: Granted executor ID app-20130610071344-0000/2 on host ip-10-170-19-137.ec2.internal with 8 cores, 40.0 GB RAM
13/06/10 07:13:44 INFO Client$ClientActor: Executor added: app-20130610071344-0000/3 on worker-20130610071332-ip-10-159-40-48.ec2.internal-59690 (ip-10-159-40-48.ec2.internal) with 8 cores
13/06/10 07:13:44 INFO SparkDeploySchedulerBackend: Granted executor ID app-20130610071344-0000/3 on host ip-10-159-40-48.ec2.internal with 8 cores, 40.0 GB RAM
13/06/10 07:13:44 INFO Client$ClientActor: Executor updated: app-20130610071344-0000/0 is now RUNNING
13/06/10 07:13:44 INFO Client$ClientActor: Executor updated: app-20130610071344-0000/3 is now RUNNING
13/06/10 07:13:44 INFO Client$ClientActor: Executor updated: app-20130610071344-0000/1 is now RUNNING
13/06/10 07:13:44 INFO Client$ClientActor: Executor updated: app-20130610071344-0000/2 is now RUNNING
mapped to array
parallelized array
13/06/10 07:13:44 INFO SparkContext: Starting job: collect at DFC_ALS.scala:134
13/06/10 07:13:44 INFO DAGScheduler: Got job 0 (collect at DFC_ALS.scala:134) with 4 output partitions (allowLocal=false)
13/06/10 07:13:44 INFO DAGScheduler: Final stage: Stage 0 (mapPartitionsWithSplit at DFC_ALS.scala:116)
13/06/10 07:13:44 INFO DAGScheduler: Parents of final stage: List()
13/06/10 07:13:44 INFO DAGScheduler: Missing parents: List()
13/06/10 07:13:44 INFO DAGScheduler: Submitting Stage 0 (MapPartitionsWithIndexRDD[1] at mapPartitionsWithSplit at DFC_ALS.scala:116), which has no missing parents
13/06/10 07:13:44 INFO DAGScheduler: Submitting 4 missing tasks from Stage 0 (MapPartitionsWithIndexRDD[1] at mapPartitionsWithSplit at DFC_ALS.scala:116)
13/06/10 07:13:44 INFO ClusterScheduler: Adding task set 0.0 with 4 tasks
13/06/10 07:13:46 INFO SparkDeploySchedulerBackend: Registered executor: Actor[akka://sparkExecutor@ip-10-171-11-51.ec2.internal:59433/user/Executor] with ID 1
13/06/10 07:13:46 INFO TaskSetManager: Starting task 0.0:0 as TID 0 on executor 1: ip-10-171-11-51.ec2.internal (preferred)
13/06/10 07:13:46 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager ip-10-171-11-51.ec2.internal:39008 with 40.4 GB RAM
13/06/10 07:13:46 INFO TaskSetManager: Serialized task 0.0:0 as 6811485 bytes in 231 ms
13/06/10 07:13:46 INFO TaskSetManager: Starting task 0.0:1 as TID 1 on executor 1: ip-10-171-11-51.ec2.internal (preferred)
13/06/10 07:13:46 INFO TaskSetManager: Serialized task 0.0:1 as 6745209 bytes in 79 ms
13/06/10 07:13:46 INFO TaskSetManager: Starting task 0.0:2 as TID 2 on executor 1: ip-10-171-11-51.ec2.internal (preferred)
13/06/10 07:13:46 INFO TaskSetManager: Serialized task 0.0:2 as 6748569 bytes in 70 ms
13/06/10 07:13:46 INFO TaskSetManager: Starting task 0.0:3 as TID 3 on executor 1: ip-10-171-11-51.ec2.internal (preferred)
13/06/10 07:13:46 INFO TaskSetManager: Serialized task 0.0:3 as 6729333 bytes in 69 ms
13/06/10 07:13:46 INFO SparkDeploySchedulerBackend: Registered executor: Actor[akka://sparkExecutor@ip-10-159-40-48.ec2.internal:47138/user/Executor] with ID 3
13/06/10 07:13:46 INFO SparkDeploySchedulerBackend: Registered executor: Actor[akka://sparkExecutor@ip-10-170-19-137.ec2.internal:38802/user/Executor] with ID 2
13/06/10 07:13:46 INFO SparkDeploySchedulerBackend: Registered executor: Actor[akka://sparkExecutor@ip-10-232-20-48.ec2.internal:52259/user/Executor] with ID 0
13/06/10 07:13:46 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager ip-10-159-40-48.ec2.internal:48767 with 40.4 GB RAM
13/06/10 07:13:46 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager ip-10-170-19-137.ec2.internal:55885 with 40.4 GB RAM
13/06/10 07:13:46 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager ip-10-232-20-48.ec2.internal:34700 with 40.4 GB RAM
13/06/10 07:14:46 INFO TaskSetManager: Finished TID 0 in 60387 ms (progress: 1/4)
13/06/10 07:14:46 INFO TaskSetManager: Finished TID 2 in 60082 ms (progress: 2/4)
13/06/10 07:14:46 INFO DAGScheduler: Completed ResultTask(0, 0)
13/06/10 07:14:46 INFO DAGScheduler: Completed ResultTask(0, 2)
13/06/10 07:14:46 INFO TaskSetManager: Finished TID 3 in 60021 ms (progress: 3/4)
13/06/10 07:14:46 INFO DAGScheduler: Completed ResultTask(0, 3)
13/06/10 07:14:46 INFO TaskSetManager: Finished TID 1 in 60179 ms (progress: 4/4)
13/06/10 07:14:46 INFO DAGScheduler: Completed ResultTask(0, 1)
13/06/10 07:14:46 INFO DAGScheduler: Stage 0 (mapPartitionsWithSplit at DFC_ALS.scala:116) finished in 61.847 s
13/06/10 07:14:46 INFO SparkContext: Job finished: collect at DFC_ALS.scala:134, took 61.883753128 s
Running MF projection
DFC complete!
Training Error: 1.243810768523837
Partition-4splits Testing Error: 1.5733611946578152
Projection-4splits Testing Error: 1.4473758667890912
Elapsed Time without load: 64 seconds
13/06/10 07:14:50 INFO MapOutputTrackerActor: MapOutputTrackerActor stopped!
13/06/10 07:14:50 INFO ConnectionManager: Selector thread was interrupted!
13/06/10 07:14:50 INFO ConnectionManager: ConnectionManager stopped
13/06/10 07:14:50 INFO MemoryStore: MemoryStore cleared
13/06/10 07:14:50 INFO BlockManager: BlockManager stopped
13/06/10 07:14:50 INFO BlockManagerMasterActor: Stopping BlockManagerMaster
13/06/10 07:14:50 INFO BlockManagerMaster: BlockManagerMaster stopped
13/06/10 07:14:50 INFO HttpService: Stopped
13/06/10 07:14:50 INFO SprayCanRootService: spray RootService stopped
[INFO] [06/10/2013 07:14:50.333] [spray-io-worker-0] [IoWorker] IoWorker thread 'spray-io-worker-0' stopped
13/06/10 07:14:50 INFO SparkContext: Successfully stopped SparkContext
[0m[[32msuccess[0m] [0mTotal time: 73 s, completed Jun 10, 2013 7:14:50 AM[0m
